{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paramiko\n",
      "  Downloading https://files.pythonhosted.org/packages/06/1e/1e08baaaf6c3d3df1459fd85f0e7d2d6aa916f33958f151ee1ecc9800971/paramiko-2.7.1-py2.py3-none-any.whl (206kB)\n",
      "Collecting bcrypt>=3.1.3 (from paramiko)\n",
      "  Downloading https://files.pythonhosted.org/packages/60/74/77573c2ee48a7f9fcd72677c43d7be1958db2c62a27602c511907d4dc058/bcrypt-3.1.7-cp36-cp36m-win_amd64.whl\n",
      "Collecting pynacl>=1.0.1 (from paramiko)\n",
      "  Downloading https://files.pythonhosted.org/packages/b5/0f/5f21bd04c8f45685c7b72013fc8efaf0e25baf3b77e62b3b915b5b43a7b6/PyNaCl-1.3.0-cp36-cp36m-win_amd64.whl (188kB)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from paramiko) (2.5)\n",
      "Requirement already satisfied: cffi>=1.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from bcrypt>=3.1.3->paramiko) (1.12.1)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from bcrypt>=3.1.3->paramiko) (1.12.0)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from cryptography>=2.5->paramiko) (0.24.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko) (2.19)\n",
      "Installing collected packages: bcrypt, pynacl, paramiko\n",
      "Successfully installed bcrypt-3.1.7 paramiko-2.7.1 pynacl-1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Install libs\n",
    "import sys\n",
    "!{sys.executable} -m pip install paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read access details \n",
    "import json\n",
    "with open('config.json') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to server ps: make sure VPN is on \n",
    "import paramiko\n",
    "client = paramiko.SSHClient()\n",
    "client.load_system_host_keys()\n",
    "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "client.connect(data['host'], username=data['username'], password=data['password'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Better Method \n",
    "sftp_client = client.open_sftp()\n",
    "remote_file = sftp_client.open('./wikidump/enwiki-20190201-pages-meta-history27.xml-p53988643p54279382','rb')\n",
    "try:\n",
    "    for line in remote_file:\n",
    "        line = line.decode(\"utf8\", \"ignore\").strip()\n",
    "        if line == \"<page>\":\n",
    "            page_text = \"\"\n",
    "            page_text += \"\\n\" + line\n",
    "            if line == \"</page>\":\n",
    "                revisions_list = []\n",
    "                page_tree = bs4.BeautifulSoup(page_text, \"html.parser\")\n",
    "                previous_text = \"\"\n",
    "                for revision_tag in page_tree.find_all(\"revision\"):\n",
    "                    revision_text = revision_tag.find_all(\"text\")[0].text\n",
    "                    if previous_text:\n",
    "                        a = [t for t in self._wikitext_segmenter(previous_text) if t]\n",
    "                        b = [t for t in self._wikitext_segmenter(revision_text) if t]\n",
    "                        s = difflib.SequenceMatcher(None, a, b)\n",
    "                        for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "                            if tag == \"equal\":\n",
    "                                continue\n",
    "                            revisions_list.append({\n",
    "                                \"old_value\": a[i1:i2],\n",
    "                                \"new_value\": b[j1:j2],\n",
    "                                \"left_context\": a[i1 - self.REVISION_WINDOW_SIZE:i1],\n",
    "                                \"right_context\": a[i2:i2 + self.REVISION_WINDOW_SIZE]\n",
    "                            })\n",
    "                    previous_text = revision_text\n",
    "                if revisions_list:\n",
    "                    page_counter += 1\n",
    "                    if self.VERBOSE and page_counter % 100 == 0:\n",
    "                        for entry in revisions_list:\n",
    "                            print(\"----------Page Counter:---------\\n\", page_counter,\n",
    "                                  \"\\n----------Old Value:---------\\n\", entry[\"old_value\"],\n",
    "                                  \"\\n----------New Value:---------\\n\", entry[\"new_value\"],\n",
    "                                  \"\\n----------Left Context:---------\\n\", entry[\"left_context\"],\n",
    "                                  \"\\n----------Right Context:---------\\n\", entry[\"right_context\"],\n",
    "                                  \"\\n==============================\")\n",
    "                    json.dump(revisions_list, open(os.path.join(rdd_folder_path, page_tree.id.text + \".json\"), \"w\"))\n",
    "finally:\n",
    "    remote_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _wikitext_segmenter(wikitext):\n",
    "        \"\"\"\n",
    "        This method takes a Wikipedia page revision text in wikitext and segments it recursively.\n",
    "        \"\"\"\n",
    "        def recursive_segmenter(node):\n",
    "            if isinstance(node, str):\n",
    "                segments_list.append(node)\n",
    "            # elif isinstance(node, mwparserfromhell.nodes.text.Text):\n",
    "            #     segments_list.append(node.value)\n",
    "            elif not node:\n",
    "                pass\n",
    "            elif isinstance(node, mwparserfromhell.wikicode.Wikicode):\n",
    "                for n in node.nodes:\n",
    "                    if isinstance(n, str):\n",
    "                        recursive_segmenter(n)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.text.Text):\n",
    "                        recursive_segmenter(n.value)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.heading.Heading):\n",
    "                        recursive_segmenter(n.title)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.tag.Tag):\n",
    "                        recursive_segmenter(n.contents)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.wikilink.Wikilink):\n",
    "                        if n.text:\n",
    "                            recursive_segmenter(n.text)\n",
    "                        else:\n",
    "                            recursive_segmenter(n.title)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.external_link.ExternalLink):\n",
    "                        # recursive_parser(n.url)\n",
    "                        recursive_segmenter(n.title)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.template.Template):\n",
    "                        recursive_segmenter(n.name)\n",
    "                        for p in n.params:\n",
    "                            # recursive_parser(p.name)\n",
    "                            recursive_segmenter(p.value)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.html_entity.HTMLEntity):\n",
    "                        segments_list.append(n.normalize())\n",
    "                    elif not n or isinstance(n, mwparserfromhell.nodes.comment.Comment) or \\\n",
    "                            isinstance(n, mwparserfromhell.nodes.argument.Argument):\n",
    "                        pass\n",
    "                    else:\n",
    "                        sys.stderr.write(\"Inner layer unknown node found: {}, {}\\n\".format(type(n), n))\n",
    "            else:\n",
    "                sys.stderr.write(\"Outer layer unknown node found: {}, {}\\n\".format(type(node), node))\n",
    "\n",
    "        segments_list = []\n",
    "        parsed_wikitext = mwparserfromhell.parse(wikitext)\n",
    "        recursive_segmenter(parsed_wikitext)\n",
    "        return segments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def extract_revisions(self, wikipedia_dumps_folder):\n",
    "        \"\"\"\n",
    "        This method takes the folder path of Wikipedia page revision history dumps and extracts the value-based corrections.\n",
    "        \"\"\"\n",
    "        rd_folder_path = os.path.join(wikipedia_dumps_folder, \"revision-data\")\n",
    "        if not os.path.exists(rd_folder_path):\n",
    "            os.mkdir(rd_folder_path)\n",
    "        compressed_dumps_list = os.listdir(wikipedia_dumps_folder)\n",
    "        page_counter = 0\n",
    "        for file_name in compressed_dumps_list:\n",
    "            compressed_dump_file_path = os.path.join(wikipedia_dumps_folder, file_name)\n",
    "            if not compressed_dump_file_path.endswith(\".7z\"):\n",
    "                continue\n",
    "            dump_file_name, _ = os.path.splitext(os.path.basename(compressed_dump_file_path))\n",
    "            rdd_folder_path = os.path.join(rd_folder_path, dump_file_name)\n",
    "            if not os.path.exists(rdd_folder_path):\n",
    "                os.mkdir(rdd_folder_path)\n",
    "            else:\n",
    "                continue\n",
    "            decompressed_dump_file_path = os.path.splitext(compressed_dump_file_path)[0]\n",
    "            # TODO use only one of these\n",
    "            with libarchive.public.file_reader(compressed_dump_file_path) as e:\n",
    "                for entry in e:\n",
    "                    with open(decompressed_dump_file_path, \"wb\") as fh:\n",
    "                        for block in entry.get_blocks():\n",
    "                            fh.write(block)\n",
    "            # compressed_dump_file = io.open(compressed_dump_file_path, \"rb\")\n",
    "            # compressed_dump = py7zlib.Archive7z(compressed_dump_file)\n",
    "            # decompressed_dump_file = io.open(decompressed_dump_file_path, \"w\")\n",
    "            # for f in compressed_dump.getmembers():\n",
    "            #    data = f.read().decode(\"utf-8\")\n",
    "            #    decompressed_dump_file.write(data)\n",
    "            # decompressed_dump_file.close()\n",
    "            decompressed_dump_file = io.open(decompressed_dump_file_path, \"r\", encoding=\"utf-8\")\n",
    "            page_text = \"\"\n",
    "            for i, line in enumerate(decompressed_dump_file):\n",
    "                line = line.strip()\n",
    "                if line == \"<page>\":\n",
    "                    page_text = \"\"\n",
    "                page_text += \"\\n\" + line\n",
    "                if line == \"</page>\":\n",
    "                    revisions_list = []\n",
    "                    page_tree = bs4.BeautifulSoup(page_text, \"html.parser\")\n",
    "                    previous_text = \"\"\n",
    "                    for revision_tag in page_tree.find_all(\"revision\"):\n",
    "                        revision_text = revision_tag.find_all(\"text\")[0].text\n",
    "                        if previous_text:\n",
    "                            a = [t for t in self._wikitext_segmenter(previous_text) if t]\n",
    "                            b = [t for t in self._wikitext_segmenter(revision_text) if t]\n",
    "                            s = difflib.SequenceMatcher(None, a, b)\n",
    "                            for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "                                if tag == \"equal\":\n",
    "                                    continue\n",
    "                                revisions_list.append({\n",
    "                                    \"old_value\": a[i1:i2],\n",
    "                                    \"new_value\": b[j1:j2],\n",
    "                                    \"left_context\": a[i1 - self.REVISION_WINDOW_SIZE:i1],\n",
    "                                    \"right_context\": a[i2:i2 + self.REVISION_WINDOW_SIZE]\n",
    "                                })\n",
    "                        previous_text = revision_text\n",
    "                    if revisions_list:\n",
    "                        page_counter += 1\n",
    "                        if self.VERBOSE and page_counter % 100 == 0:\n",
    "                            for entry in revisions_list:\n",
    "                                print(\"----------Page Counter:---------\\n\", page_counter,\n",
    "                                      \"\\n----------Old Value:---------\\n\", entry[\"old_value\"],\n",
    "                                      \"\\n----------New Value:---------\\n\", entry[\"new_value\"],\n",
    "                                      \"\\n----------Left Context:---------\\n\", entry[\"left_context\"],\n",
    "                                      \"\\n----------Right Context:---------\\n\", entry[\"right_context\"],\n",
    "                                      \"\\n==============================\")\n",
    "                        json.dump(revisions_list, open(os.path.join(rdd_folder_path, page_tree.id.text + \".json\"), \"w\"))\n",
    "            decompressed_dump_file.close()\n",
    "            os.remove(decompressed_dump_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Method \n",
    "from bs4 import BeautifulSoup\n",
    "import select\n",
    "transport = client.get_transport()\n",
    "channel = transport.open_session()\n",
    "channel.exec_command(\"cat ./wikidump/enwiki-20190201-pages-meta-history27.xml-p53988643p54279382\")\n",
    "#while True:\n",
    "    rl, wl, xl = select.select([channel],[],[],0.0)\n",
    "    if len(rl) > 0:\n",
    "        #print(channel.recv(1024))\n",
    "        soup = BeautifulSoup(channel.recv(16384), 'html.parser')\n",
    "        #print(soup.prettify())\n",
    "        print(soup.find_all('page'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://stackoverflow.com/questions/1596963/read-a-file-from-server-with-ssh-using-python\n",
    "- https://github.com/BigDaMa/baran/blob/master/baran.py\n",
    "(\"_wikitext_segmenter\" and \"extract_revisions\" methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
